<!DOCTYPE html>

<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->
    <meta name="description" content="Hanzhe Hu‘s Homepage">
    <meta name="author" content="Hanzhe Hu">
    <link rel="icon" href="./files/SCS-Dragon.jpeg">

    <title>Hanzhe Hu</title>

    <!-- Bootstrap core CSS -->
    <link href="./files/bootstrap.min.css" rel="stylesheet">
    <link href="./files/hzhu-homepage.css" rel="stylesheet">
</head>

<body>

<nav class="navbar navbar-inverse navbar-fixed-top" style="background-color: #8f1814">
    <div class="container" style="background-color: #8f1814">
        <div class="navbar-header ">
            <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="https://hzhupku.github.io/"><strong>Hanzhe Hu</strong></a>
        </div>
        <div id="navbar" class="collapse navbar-collapse">
            <ul class="nav navbar-nav">
                <li>
                    <a href="https://hzhupku.github.io/">Home</a>
                </li>
                <li>
                    <a href="https://hzhupku.github.io/#about">About</a>
                </li>
                <li>
                    <a href="https://hzhupku.github.io/#publications">Publications</a>
                </li>
                <li>
                    <a href="https://hzhupku.github.io/#research">Research</a>
                </li>
                <li>
                    <a target="_blank" href="./files/CV_Hanzhe.pdf">CV</a>
                </li>
                
            </ul>
        </div><!--/.nav-collapse -->
    </div>
</nav><!-- Fixed navbar-collapse -->

<!-- Begin page content -->
<div class="container">
    <!--Body content-->
    <div class="starter-template">

        <div class="row row-offcanvas row-offcanvas-right">

            <div class="col-xs-6 col-sm-3"><img alt="Beijing, 2022" src="./files/hhz_202211.jpeg" height="290px" width="240px" class="img-rounded"><br>
            </a>
            </div>

            <div class="col-xs-12 col-sm-8">
                <h1>
                    <strong>Hanzhe Hu</strong><br>
                    <small>Robotics Institute, School of Computer Science</small><br>
                    <small>Carnegie Mellon University</small>
                </h1>
                <p>
                    Smith Hall (EDSH)<br>
                    5000 Forbes Ave<br>
                    <!--5th Floor, Office 506<br>-->
                    Carnegie Mellon University<br>
                    Pittsburgh, PA, 15213, US
                </p>

                <p>Email:<code>hanzheh[at]cs.cmu.edu</code><br>
                    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<code>hanzhehu[at]cmu.edu</code><br>
                    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<code>huhz[at]pku.edu.cn</code></p>
            </div>
            <!-- <div class="col-xs-2 col-sm-1"></div> -->
            <!-- <br> -->
        </div>


        <hr size="1" color="#800000">
        <div id="about">
            <p style="text-align:justify; max-width:97%">I am a PhD student at <a target="_blank" href="https://www.cs.cmu.edu/">School of Computer Science</a> of <a target="_blank" href="https://www.cmu.edu/">Carnegie Mellon University</a>, adviced by Prof. <a target="_blank" href="https://shubhtuls.github.io/">Shubham Tulsiani</a>. 
            
            I obtained my Master's degree in Computer Science from School of EECS 
            of <a target="_blank" href="http://english.pku.edu.cn/">Peking University</a>, 
            adviced by Prof. <a target="_blank" href="http://www.liweiwang-pku.com/">Liwei Wang</a>.
            Prior to PKU, I got my Bachelor's degree in Physics from <a target="_blank" href="http://www.nju.edu.cn/">Nanjing University</a>
            and worked at <a target="_blank" href="http://www.lamda.nju.edu.cn/MainPage.ashx">LAMDA</a> of NJU led by Prof. <a target="_blank" href="https://cs.nju.edu.cn/zhouzh/">Zhihua Zhou</a>. I spent a wonderful summer in 2021 doing internship at <a target="_blank" href="https://ucsd.edu/">UCSD</a>, adviced by Prof. <a target="_blank" href="https://xiaolonw.github.io/">Xiaolong Wang</a>.</p> 

            <p style="text-align:justify; max-width:97%">My research interests are Computer Vision and Machine Learning. Especially, I am interested in 3D Vision, Scene Understanding and Label-Efficient Learning. 
            <!--the following topics:
            <ul> 
                <li style="font-size:130%"> 3D Vision </li>
                <li style="font-size:130%"> Scene Understanding </li>
                <li style="font-size:130%"> Label-Efficient Learning </li>
            </ul>
        -->
            </p>

            <p> Here is my <a target="_blank" href="./files/CV_Hanzhe.pdf">resume</a>.
            You can also view my <a href="https://scholar.google.com/citations?user=LjTCVxAAAAAJ&hl=en">Google Scholar profile</a>. </p>
            <p> I am always open to research collaborations. Feel free to contact me.</p>
        </div>
        <!--<h1 class="text-hide"><a id="about">Custom heading</a></h1>-->


        <hr size="2" color="#800000">
        <h2><b><a id="news">What's New</a></b></h2>

        <!-- Stack the columns on mobile by making one full-width and the other half-width -->
        <div class="row">
        <div class="col-xs-2 col-md-2"><p align="center"><strong>&#160;08/15/2022</strong></p></div>
        <div class="col-xs-18 col-md-19"><p>I started my PhD at CMU.</p></div>
        <div class="col-xs-2 col-md-2"><p align="center"><strong>&#160;07/04/2022</strong></p></div>
        <div class="col-xs-18 col-md-19"><p>One paper is accepted by ECCV2022!</p></div>
        <div class="col-xs-2 col-md-2"><p align="center"><strong>&#160;06/18/2022</strong></p></div>
        <div class="col-xs-18 col-md-19"><p>I graduated from Peking University!</p></div>
        <div class="col-xs-2 col-md-2"><p align="center"><strong>&#160;11/18/2021</strong></p></div>
        <div class="col-xs-18 col-md-19"><p>Two papers are submitted to CVPR2022.</p></div>
        <div class="col-xs-2 col-md-2"><p align="center"><strong>&#160;09/29/2021</strong></p></div>
        <div class="col-xs-18 col-md-19"><p>One paper is accepted by NeurIPS2021 as Spotlight!</p></div>
        <div class="col-xs-2 col-md-2"><p align="center"><strong>&#160;07/23/2021</strong></p></div>
        <div class="col-xs-18 col-md-19"><p>One paper is accepted by ICCV2021!</p></div>
        <div class="col-xs-2 col-md-2"><p align="center"><strong>&#160;07/04/2021</strong></p></div>
        <div class="col-xs-18 col-md-19"><p>I am selected as "<strong>Top 10 Outstanding Researcher</strong>" (学术十杰), EECS of Peking University.</p></div>
        <div class="col-xs-2 col-md-2"><p align="center"><strong>&#160;05/28/2021</strong></p></div>
        <div class="col-xs-18 col-md-19"><p>One paper is submitted to NeurIPS2021.</p></div>
        <div class="col-xs-2 col-md-2"><p align="center"><strong>&#160;03/01/2021</strong></p></div>
        <div class="col-xs-18 col-md-19"><p>One paper is accepted by CVPR2021!</p></div>
        <div class="col-xs-2 col-md-2"><p align="center"><strong>&#160;12/02/2020</strong></p></div>
        <div class="col-xs-18 col-md-19"><p>One paper is accepted by AAAI2021!</p></div>
        <div class="col-xs-2 col-md-2"><p align="center"><strong>&#160;07/03/2020</strong></p></div>
        <div class="col-xs-18 col-md-19"><p>One paper is accepted by ECCV2020!</p></div>   
        <div class="col-xs-2 col-md-2"><p align="center"><strong>&#160;06/21/2020</strong></p></div>
        <div class="col-xs-18 col-md-19"><p>One paper is accepted by ICPR2020.</p></div>     
        <div class="col-xs-2 col-md-2"><p align="center"><strong>&#160;02/01/2020</strong></p></div>
        <div class="col-xs-18 col-md-19"><p>One paper is accepted by CVPR2020!</p></div>
    
        </div>
      

        
        <!-- <dl class="dl-horizontal"> -->
            <!-- <ul style="list-style:none; margin:0px; padding:0px;"> --> 
            <!-- <dt>09 / 2019</dt> -->
            <!-- <dd>My homepage is built today!</dd>                -->
            <!-- </ul> -->

            <div align="right">
                <button type="button" class="btn btn-success btn-xs" data-target="#old-news" data-toggle="collapse">Older news</button>
            </div>
        </dl>
        <div id="old-news" class="collapse">
                <dl>
            </dl>
        </div>


        <hr size="1" color="#800000">
        <h2><b><a id="publications">Publications</a></b></h2>



        <h3>Conference Paper</h3>

        <div class="panel panel-default">
            <table class="table table-hover">
                <tbody>
                <!--<tr>
                    <td>
                        <span class="label label-primary">C5</span>
                    </td>
                    <td>
                        <h4 class="list-group-item-heading">
                            Meta-Seg: Towards General Few-shot Segmentation
                            <span class="badge"><a class="conf" target="_blank" href="http://cvpr2021.thecvf.com/">CVPR '21</a></span>
                        </h4>
                        <p class="author">
                            <strong>Hanzhe Hu</strong>, Yiru Wang, Weihao Gan, Deyi Ji, Wei Wu, Junjie Yan, Jinshi Cui, Liwei Wang<br>
                            Under Review.<br>
                            
                        </p>
                        <button type="button" class="btn btn-success btn-xs" data-target="#cvpr21-abs" data-toggle="collapse">Abstract</button>
                        <button type="button" class="btn btn-info btn-xs" data-target="#cvpr21-bib" data-toggle="collapse">Bib</button>
                        <button type="button" class="btn btn-primary btn-xs"><a class="conf" target="_blank" href="./files/cvpr21.pdf">PDF</a></button>
                        <p></p>
                        <div id="cvpr21-abs" class="collapse">

                        </div>
                        <div id="cvpr21-bib" class="collapse">
            
                        </div>
                    </td>
                </tr>-->
                <tr>
                    <td>
                        <span class="label label-primary">C8</span>
                    </td>
                    <td>
                        <h4 class="list-group-item-heading">
                            Learning Implicit Feature Alignment Function for Semantic Segmentation
                            <span class="badge"><a class="conf" target="_blank" href="https://eccv2022.ecva.net/">ECCV '22</a></span>
                        </h4>
                        <p class="author">
                            <strong>Hanzhe Hu</strong>, Yinbo Chen, Jiarui Xu, Shubhankar Borse, Hong Cai, Fatih Porikli and Xiaolong Wang<br>
                            In Proceedings of European Conference on Computer Vision.<br>
                            
                        </p>
                        <button type="button" class="btn btn-success btn-xs" data-target="#eccv22-abs" data-toggle="collapse">Abstract</button>
                        <button type="button" class="btn btn-info btn-xs" data-target="#eccv22-bib" data-toggle="collapse">Bib</button>
                        <button type="button" class="btn btn-primary btn-xs"><a class="conf" target="_blank" href="./files/eccv22.pdf">PDF</a></button>
                        <p></p>
                        <div id="eccv22-abs" class="collapse">
                            Integrating high-level context information with low-level details is of central importance in semantic segmentation. Towards this end, most existing segmentation models apply bilinear up-sampling and
                            convolutions to feature maps of different scales, and then align them at the same resolution. However, bilinear up-sampling blurs the precise information learned in these feature maps and convolutions incur extra computation costs. To address these issues, we propose the Implicit Feature Alignment function (IFA). Our method is inspired by the rapidly expanding topic of implicit neural representations, where coordinatebased neural networks are used to designate fields of signals. In IFA, feature vectors are viewed as representing a 2D field of information. Given a query coordinate, nearby feature vectors with their relative coordinates are taken from the multi-level feature maps and then fed into an MLP to generate the corresponding output. As such, IFA implicitly aligns the feature maps at different levels and is capable of producing segmentation maps in arbitrary resolutions. We demonstrate the efficacy of IFA on multiple datasets, including Cityscapes, PASCAL Context, and ADE20K. Our method can be combined with improvement
                            on various architectures, and it achieves state-of-the-art computationaccuracy trade-off on common benchmarks. Code will be made available at https://github.com/hzhupku/IFA.
                        </div>
                        <div id="eccv22-bib" class="collapse">
            
                        </div>
                    </td>
                </tr>
                <tr>
                    <td>
                        <span class="label label-primary">C7</span>
                    </td>
                    <td>
                        <h4 class="list-group-item-heading">
                            Semi-Supervised Semantic Segmentation via Adaptive Equalization Learning
                            <span class="badge"><a class="conf" target="_blank" href="https://nips.cc/">NeurIPS '21</a></span>
                            <span class="badge"><a class="conf" target="_blank">Spotlight</a></span>
                        </h4>
                        <p class="author">
                            <strong>Hanzhe Hu</strong>, Fangyun Wei, Han Hu, Qiwei Ye, Jinshi Cui, Liwei Wang<br>
                            Advances in Neural Information Processing Systems.<br>
                            
                        </p>
                        <button type="button" class="btn btn-success btn-xs" data-target="#nips21-abs" data-toggle="collapse">Abstract</button>
                        <button type="button" class="btn btn-info btn-xs" data-target="#nips21-bib" data-toggle="collapse">Bib</button>
                        <button type="button" class="btn btn-primary btn-xs"><a class="conf" target="_blank" href="./files/nips21.pdf">PDF</a></button>
                        <p></p>
                        <div id="nips21-abs" class="collapse">
                            Due to the limited and even imbalanced data, semi-supervised semantic segmentation tends to have poor performance on some certain categories, e.g., tailed categories in Cityscapes dataset which exhibits a long-tailed label distribution.
                            Existing approaches almost all neglect this problem, and treat categories equally.
                            Some popular approaches such as consistency regularization or pseudo-labeling
                            may even harm the learning of under-performing categories, that the predictions or
                            pseudo labels of these categories could be too inaccurate to guide the learning on the
                            unlabeled data. In this paper, we look into this problem, and propose a novel framework for semi-supervised semantic segmentation, named adaptive equalization learning (AEL). AEL adaptively balances the training of well and badly performed
                            categories, with a confidence bank to dynamically track category-wise performance
                            during training. The confidence bank is leveraged as an indicator to tilt training
                            towards under-performing categories, instantiated in three strategies: 1) adaptive
                            Copy-Paste and CutMix data augmentation approaches which give more chance
                            for under-performing categories to be copied or cut; 2) an adaptive data sampling
                            approach to encourage pixels from under-performing category to be sampled; 3) a
                            simple yet effective re-weighting method to alleviate the training noise raised by
                            pseudo-labeling. Experimentally, AEL outperforms the state-of-the-art methods by
                            a large margin on the Cityscapes and Pascal VOC benchmarks under various data
                            partition protocols. Code is available at https://github.com/hzhupku/SemiSeg-AEL.
                        </div>
                        <div id="nips21-bib" class="collapse">
            
                        </div>
                    </td>
                </tr>
                <tr>
                    <td>
                        <span class="label label-primary">C6</span>
                    </td>
                    <td>
                        <h4 class="list-group-item-heading">
                            Region-aware Contrastive Learning for Semantic Segmentation
                            <span class="badge"><a class="conf" target="_blank" href="http://iccv2021.thecvf.com/">ICCV '21</a></span>
                        </h4>
                        <p class="author">
                            <strong>Hanzhe Hu</strong>, Jinshi Cui, Liwei Wang<br>
                            Proceedings of the International Conference in Computer Vision.<br>
                            
                        </p>
                        <button type="button" class="btn btn-success btn-xs" data-target="#iccv21-abs" data-toggle="collapse">Abstract</button>
                        <button type="button" class="btn btn-info btn-xs" data-target="#iccv21-bib" data-toggle="collapse">Bib</button>
                        <button type="button" class="btn btn-primary btn-xs"><a class="conf" target="_blank" href="./files/iccv21.pdf">PDF</a></button>
                        <p></p>
                        <div id="iccv21-abs" class="collapse">
                            Recent works have made great success in semantic seg- mentation by exploiting contextual information in a local or global manner within individual image and supervis- ing the model with pixel-wise cross entropy loss. However, from the holistic view of the whole dataset, semantic rela- tions not only exist inside one single image, but also prevail in the whole training data, which makes solely consider- ing intra-image correlations insufficient. Inspired by recent progress in unsupervised contrastive learning, we propose the region-aware contrastive learning (RegionContrast) for semantic segmentation in the supervised manner. In or- der to enhance the similarity of semantically similar pix- els while keeping the discrimination from others, we em- ploy contrastive learning to realize this objective. With the help of memory bank, we explore to store all the represen- tative features into the memory. Without loss of generality, to efficiently incorporate all training data into the memory bank while avoiding taking too much computation resource, we propose to construct region centers to represent features from different categories for every image. Hence, the pro- posed region-aware contrastive learning is performed in a region level for all the training data, which saves much more memory than methods exploring the pixel-level rela- tions. The proposed RegionContrast brings little computa- tion cost during training and requires no extra overhead for testing. Extensive experiments demonstrate that our method achieves state-of-the-art performance on three benchmark datasets including Cityscapes, ADE20K and COCO Stuff.
                        </div>
                        <div id="iccv21-bib" class="collapse">
                            @InProceedings{Hu_2021_ICCV,
                                author    = {Hu, Hanzhe and Cui, Jinshi and Wang, Liwei},
                                title     = {Region-Aware Contrastive Learning for Semantic Segmentation},
                                booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
                                month     = {October},
                                year      = {2021},
                                pages     = {16291-16301}
                            }
                        </div>
                    </td>
                </tr>
                <tr>
                    <td>
                        <span class="label label-primary">C5</span>
                    </td>
                    <td>
                        <h4 class="list-group-item-heading">
                            Dense Relation Distillation with Context-aware Aggregation for Few-Shot Object Detection
                            <span class="badge"><a class="conf" target="_blank" href="http://cvpr2021.thecvf.com/">CVPR '21</a></span>
                        </h4>
                        <p class="author">
                            <strong>Hanzhe Hu</strong>,  Shuai Bai, Aoxue Li, Jinshi Cui, Liwei Wang<br>
                            Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.<br>
                            
                        </p>
                        <button type="button" class="btn btn-success btn-xs" data-target="#cvpr21-abs" data-toggle="collapse">Abstract</button>
                        <button type="button" class="btn btn-info btn-xs" data-target="#cvpr21-bib" data-toggle="collapse">Bib</button>
                        <button type="button" class="btn btn-primary btn-xs"><a class="conf" target="_blank" href="./files/cvpr21.pdf">PDF</a></button>
                        <p></p>
                        <div id="cvpr21-abs" class="collapse">
                            Conventional deep learning based methods for object detection require a large amount of bounding box annotations for training, which is expensive to obtain such high quality annotated data. Few-shot object detection, which learns to adapt to novel classes with only a few annotated examples, is very challenging since the fine-grained feature of novel object can be easily overlooked with only a few data available. In this work, aiming to fully exploit features of annotated novel object and capture fine-grained features of query object, we propose Dense Relation Distillation with Context-aware Aggregation (DCNet) to tackle the few-shot detection problem. Built on the meta-learning based framework, Dense Relation Distillation module targets at fully exploiting support features, where support features and query feature are densely matched, covering all spatial locations in a feed-forward fashion. The abundant usage of the guidance information endows model the capability to handle common challenges such as appearance changes and occlusions. Moreover, to better capture scale-aware features, Context-aware Aggregation module adaptively harnesses features from different scales for a more comprehensive feature representation. Extensive experiments illustrate that our proposed approach achieves state-of-the-art results on PASCAL VOC and MS COCO datasets. Code will be made available at https://github.com/hzhupku/DCNet.
                        </div>
                        <div id="cvpr21-bib" class="collapse">
            
                        </div>
                    </td>
                </tr>
                <tr>
                    <td>
                        <span class="label label-primary">C4</span>
                    </td>
                    <td>
                        <h4 class="list-group-item-heading">
                            Context-aware Graph Convolution Network for Target Re-identification
                            <span class="badge"><a class="conf" target="_blank" href="https://aaai.org/Conferences/AAAI-21/">AAAI '21</a></span>
                        </h4>
                        <p class="author">
                            Deyi Ji, Haoran Wang, <strong>Hanzhe Hu</strong>,  Weihao Gan, Wei Wu, Junjie Yan<br>
                            In Proceedings of AAAI Conference on Aritifical Intelligence.<br>
                            
                        </p>
                        <button type="button" class="btn btn-success btn-xs" data-target="#aaai21-abs" data-toggle="collapse">Abstract</button>
                        <button type="button" class="btn btn-info btn-xs" data-target="#aaai21-bib" data-toggle="collapse">Bib</button>
                        <button type="button" class="btn btn-primary btn-xs"><a class="conf" target="_blank" href="./files/aaai21.pdf">PDF</a></button>
                        <p></p>
                        <div id="aaai21-abs" class="collapse">
                                Most existing re-identification methods focus on learning robust and discriminative features with deep convolution networks. However, many of them consider content similarity separately and fail to utilize the context information of the query and gallery sets, e.g. probe-gallery and gallery-gallery relations, thus hard samples may not be well solved due to the limited or even misleading information. In this paper, we present a novel Context-Aware Graph Convolution Network (CAGCN), where the probe-gallery relations are encoded into the graph nodes and the graph edge connections are well controlled by the gallery-gallery relations. In this way, hard samples can be addressed with the context information flows among other easy samples during the graph reasoning. Specifically, we adopt an effective hard gallery sampler to obtain high recall for positive samples while keeping a reasonable graph size, which can also weaken the imbalanced problem in training process with low computation complexity. Experiments show that the proposed method achieves state-of-the-art performance on both person and vehicle re-identification datasets in a plug and play fashion with limited overhead.
                        </div>
                        <div id="aaai21-bib" class="collapse">
            
                        </div>
                    </td>
                </tr>
                <tr>
                    <td>
                        <span class="label label-primary">C3</span>
                    </td>
                    <td>
                        <h4 class="list-group-item-heading">
                            Class-wise Dynamic Graph Convolution for Semantic Segmentation
                            <span class="badge"><a class="conf" target="_blank" href="https://eccv2020.eu/">ECCV '20</a></span>
                        </h4>
                        <p class="author">
                            <strong>Hanzhe Hu</strong>, Deyi Ji, Weihao Gan, Shuai Bai, Wei Wu, Junjie Yan<br>
                            In Proceedings of European Conference on Computer Vision.<br>
                        </p>
                        <button type="button" class="btn btn-success btn-xs" data-target="#eccv20-abs" data-toggle="collapse">Abstract</button>
                        <button type="button" class="btn btn-info btn-xs" data-target="#eccv20-bib" data-toggle="collapse">Bib</button>
                        <button type="button" class="btn btn-primary btn-xs"><a class="conf" target="_blank" href="./files/eccv20.pdf">PDF</a></button>
                        <p></p>
                        <div id="eccv20-abs" class="collapse">
                            Recent works have made great progress in semantic segmentation by exploiting contextual information in a local or global manner
                            with dilated convolutions, pyramid pooling or self-attention mechanism.
                            In order to avoid potential misleading contextual information aggregation in previous works, we propose a class-wise dynamic graph convolution(CDGC) module to adaptively propagate information. The graph
                            reasoning is performed among pixels in the same class. Based on the
                            proposed CDGC module, we further introduce the Class-wise Dynamic
                            Graph Convolution Network(CDGCNet), which consists of two main
                            parts including the CDGC module and a basic segmentation network,
                            forming a coarse-to-fine paradigm. Specifically, the CDGC module takes
                            the coarse segmentation result as class mask to extract node features
                            for graph construction and performs dynamic graph convolutions on the
                            constructed graph to learn the feature aggregation and weight allocation.
                            Then the refined feature and the original feature are fused to get the final
                            prediction. We conduct extensive experiments on three popular semantic segmentation benchmarks including Cityscapes, PASCAL VOC 2012
                            and COCO Stuff, and achieve state-of-the-art performance on all three
                            benchmarks.
                        </div>
                        <div id="eccv20-bib" class="collapse">
                            @misc{hu2020classwise,
                                title={Class-wise Dynamic Graph Convolution for Semantic Segmentation},
                                author={Hanzhe Hu and Deyi Ji and Weihao Gan and Shuai Bai and Wei Wu and Junjie Yan},
                                year={2020},
                                eprint={2007.09690},
                                archivePrefix={arXiv},
                                primaryClass={cs.CV}
                            }
                        </div>
                    </td>
                </tr>


                
                <tr>
                    <td>
                        <span class="label label-primary">C2</span>
                    </td>
                    <td>
                        <h4 class="list-group-item-heading">
                            Boundary-aware Graph Convolution for Semantic Segmentation
                            <span class="badge"><a class="conf" target="_blank" href="https://www.micc.unifi.it/icpr2020/">ICPR '20</a></span>
                        </h4>
                        <p class="author">
                            <strong>Hanzhe Hu</strong>, Jinshi Cui, Hongbin Zha<br>
                            International Conference on Pattern Recognition.<br>
                        </p>
                        <button type="button" class="btn btn-success btn-xs" data-target="#icpr20-abs" data-toggle="collapse">Abstract</button>
                        <button type="button" class="btn btn-info btn-xs" data-target="#icpr20-bib" data-toggle="collapse">Bib</button>
                        <button type="button" class="btn btn-primary btn-xs"><a class="conf" target="_blank" href="./files/icpr20.pdf">PDF</a></button>
                        <p></p>
                        <div id="icpr20-abs" class="collapse">
                                Recent works have made great progress in semantic segmentation by exploiting contextual information in a local or global manner with dilated convolutions, pyramid pooling or self-attention mechanism. However, few works have focused on harvesting boundary information to improve the segmentation performance. In order to enhance the feature similarity within the object and keep discrimination from other objects, we propose a boundary-aware graph convolution (BGC) module to propagate features within the object. The graph reasoning is performed among pixels of the same object apart from the boundary pixels. Based on the proposed BGC module, we further introduce the Boundary-aware Graph Convolution Net- work(BGCNet), which consists of two main components including a basic segmentation network and the BGC module, forming a coarse-to-fine paradigm. Specifically, the BGC module takes the coarse segmentation feature map as node features and boundary prediction to guide graph construction. After graph convolution, the reasoned feature and the input feature are fused together to get the refined feature, producing the refined segmentation result. We conduct extensive experiments on three popular semantic segmentation benchmarks including Cityscapes, PASCAL VOC 2012 and COCO Stuff, and achieve state-of-the-art performance on all three benchmarks.
                        </div>
                        <div id="icpr20-bib" class="collapse">
            
                        </div>
                    </td>
                </tr>          

                    <tr>     
                    <td>
                        <span class="label label-primary">C1</span>
                    </td>
                    <td>
                        <h4 class="list-group-item-heading">
                            Adaptive Dilated Network with Self-Correction Supervision for Counting
                            <span class="badge"><a class="conf" target="_blank" href="http://cvpr2020.thecvf.com/">CVPR '20</a></span>
                        </h4>
                        <p class="author">
                            Shuai Bai, Zhiqun He, Yu Qiao, <strong>Hanzhe Hu</strong>, Wei Wu, Junjie Yan<br>
                            Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.<br>
                        </p>
                        <button type="button" class="btn btn-success btn-xs" data-target="#cvpr20-abs" data-toggle="collapse">Abstract</button>
                        <button type="button" class="btn btn-info btn-xs" data-target="#cvpr20-bib" data-toggle="collapse">Bib</button>
                        <button type="button" class="btn btn-primary btn-xs"><a class="conf" target="_blank" href="./files/cvpr20.pdf">PDF</a></button>
                        <p></p>
                        <div id="cvpr20-abs" class="collapse">
                            The counting problem aims to estimate the number of ob- jects in images. Due to large scale variation and labeling deviations, it remains a challenging task. The static den- sity map supervised learning framework is widely used in existing methods, which uses the Gaussian kernel to gen- erate a density map as the learning target and utilizes the Euclidean distance to optimize the model. However, the framework is intolerable to the labeling deviations and can not reflect the scale variation. In this paper, we propose an adaptive dilated convolution and a novel supervised learn- ing framework named self-correction (SC) supervision. In the supervision level, the SC supervision utilizes the out- puts of the model to iteratively correct the annotations and employs the SC loss to simultaneously optimize the model from both the whole and the individuals. In the feature level, the proposed adaptive dilated convolution predicts a continuous value as the specific dilation rate for each loca- tion, which adapts the scale variation better than a discrete and static dilation rate. Extensive experiments illustrate that our approach has achieved a consistent improvement on four challenging benchmarks. Especially, our approach achieves better performance than the state-of-the-art meth- ods on all benchmark datasets.
                        </div>
                        <div id="cvpr20-bib" class="collapse">
                            @InProceedings{Bai_2020_CVPR,
                                author = {Bai, Shuai and He, Zhiqun and Qiao, Yu and Hu, Hanzhe and Wu, Wei and Yan, Junjie},
                                title = {Adaptive Dilated Network With Self-Correction Supervision for Counting},
                                booktitle = {The IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
                                month = {June},
                                year = {2020}
                                }
                        </div>
                    </td>
                </tr>

                </tbody>
            </table>
        </div>

        <h3>Journal Article</h3>


        
        <h3>Thesis</h3>

        <div class="panel panel-default">
            <!--<div class="panel-body">-->
            <table class="table table-hover">
                <tbody>
                    <tr>
                        <td>
                            <span class="label label-primary">T2</span>
                        </td>
                        <td>
                            <h4 class="list-group-item-heading">
                                Label-Efficient Learning for Object Recognition.
                            </h4>
                            <p class="author">
                                <strong>Hanzhe Hu</strong><br>
                                Master's Thesis, Peking University, 2022.<br>
                            </p>                           
                            <button type="button" class="btn btn-success btn-xs" data-target="#master-thesis-abs" data-toggle="collapse">Abstract</button>
                            <div id="master-thesis-abs" class="collapse">
                                Human beings often exhibit strong performance in learning a new concept with only a few examples available. Artificial intelligence, in particular deep learning, usually requires large amounts of labeled data for training and can easily get stuck in label-scarce scenarios. Given this situation, label-efficient learning has become a vital research direction recently, which targets at learning tasks in a label efficient way through techniques such as semi-supervised learning and few-shot learning.
    
                                In this dissertation, we study the problem of label-efficient learning for object recognition, especially for semantic segmentation and object detection. The core idea is to fully exploit the limited available labels and enable the model to generalize well to target tasks. Firstly, to alleviate the prevailing imbalance problem in semi-supervised semantic segmentation, we propose a novel adaptive equalization learning framework which adaptively balances the training of well and badly performed categories, and alleviate the training noise raised by pseudo-labeling. Secondly, aiming to fully exploit available labeled data and capture fine-grained features, we propose a dense relation distillation module with context-aware aggregation to tackle the few-shot object detection problem. The abundant usage of the guidance information endows model the capability to handle common challenges such as appearance changes and occlusions. Thirdly, to realize the generalized few-shot semantic segmentation model with strong discriminative power among categories, we propose a meta learning based segmentation model for generalized N-way few-shot segmentation and a region-aware contrastive learning mechanism for supervised semantic segmentation. Empowered by the two techniques, the model is able to efficiently learn holistic context and perform effective N-way K-shot segmentation.
                                
                                Extensive experiments on the Cityscapes, PASCAL VOC, MS COCO and ADE20K benchmark datasets demonstrate that our methods outperform the state-of-the-art methods by a large margin. Specially, 1) our method on semi-supervised semantic segmentation not only achieves state-of-the-art results, but also obtains significant improvement on under-represented categories; 2) our method on few-shot object detection serves even better under extremely low-shot scenarios, indicating 
                               the ability to capture local detailed information to overcome the variations brought by the randomly sampled training shots; 3) our method on few-shot semantic segmentation outperforms well-established baseline methods by a large margin for generalized few-shot segmentation. 
                            </div>
                            <p></p>
                        </td>
                    </tr>
                    <tr>
                        <td>
                            <span class="label label-primary">T1</span>
                        </td>
                        <td>
                            <h4 class="list-group-item-heading">
                                Deep Supervised Facial Expression Recognition.
                            </h4>
                            <p class="author">
                                <strong>Hanzhe Hu</strong><br>
                                Bachelor's Thesis, Nanjing University, 2019.<br>
                            </p>                           
                            <button type="button" class="btn btn-success btn-xs" data-target="#bachelor-thesis-abs" data-toggle="collapse">Abstract</button>
                            <div id="bachelor-thesis-abs" class="collapse">
                                    As the great success of machine learning and deep learning, face recognition has
                                    achieved unprecedented development with sufficient data to assist the learning process
                                    of deep learning algorithms. The current facial expression recognition algorithms mainly focus on two important problems: the over-fitting
                                    problem caused by insufficient amount of data and the recognition deviation caused by
                                    illumination, head pose and identity bias. This paper proposes solutions for facial expression recognition tasks in laboratorycontrolled scenes and in-the-wild scenes. First of all, in the laboratory-controlled scenario, due to the lack of data, the use of deep neural networks such as resnet-101 is likely
                                    to cause over-fitting. This paper utilizes the idea of ensemble learning, using multiple convolutional neural networks as sub-classifiers and outputs the final result. This
                                    method achieves the best recognition accuracy on both the FER2013 and CK+ datasets.
                                    Secondly, in the natural scenes, this paper focuses on solving the recognition problem
                                    caused by the invariance of head poses. In former research, the use of the generative
                                    adversarial network (GAN) for this task has been put forward, but the algorithm normally ignores the expression information. In this paper, the updated encoder-decoder
                                    mechanism is used to encode the expression information into the generator, and use it
                                    to generate a complete face with expression information for the expression recognition task. This method is tested on Multi-PIE dataset and achieves very impressive results.
                            </div>
                            <p></p>
                        </td>
                    </tr>


                </tbody>
            </table>
        </div>

        <hr size="1" color="#800000">
        <h2><b><a id="research">Project</a></b></h2>

                <!-- <li class="list-group-item"> -->
                        <div class="panel panel-default">
                                <!-- <div class="panel-body"> -->
                                    <table class="table table-hover">
                                        <tbody>
                                            <!--
                                            <tr>
                                                <td>
                                                    <p></p>
                                                    <div class="row vert-offset-top-1 vert-offset-bottom-1">
                                                        <div class="col-md-4">
                                                            <img src="./files/cdgc2.png" class="img-thumbnail center-block" style="max-width:90%">
                                                        </div>
                                                        <div class="col-md-75">
                                                            <p><b>Class-wise Dynamic Graph Convolution for Semantic Segmentation</b><br>
                                                            <small>Peking University, SenseTime Group, advised by <a href="https://wuwei-ai.org/" target="_blank">Wei Wu</a></small> 
                                                            <p style="text-align:justify; max-width:97%"><small>Recent work has made great progress in semantic segmentation by exploit ing contextual information in a local or
                                                                    global manner with dilated convolutions, pyramid pooling or self-attention mechanism. In order to avoid problematic contextual information aggregation in previous work, we proposed a class-wise dynamic graph convolution(CDGC) module to adaptively propagate information. Our proposed framework achieves the state-of-art performance on three popular benchmark datasets: Cityscapes, Pascal Voc2012 and COCO Stuff. Our paper is under review in ECCV2020.</small></p>
                                                        </div>
                                                    </div>
                                                </td>
                                            </tr>
                                            -->
        
                                            <!--
                                            <tr>
                                                <td>
                                                    <p></p>
                                                    <div class="row vert-offset-top-1 vert-offset-bottom-1">
                                                        <div class="col-md-4">
                                                            <img src="./files/lane2.png" class="img-thumbnail center-block" style="max-width:90%">
                                                        </div>
                                                        <div class="col-md-75">
                                                            <p><b>Lane-line semantic and instance segmentation</b><br>
                                                            <small>SenseTime Group, advised by <a href="https://wuwei-ai.org/" target="_blank">Wei Wu</a></small> 
                                                            <p style="text-align:justify; max-width:97%"><small>In smart city project, lane-line detection is important for anomaly detection such as breaking traffic rules. Currently,
                                                                    we implement the state-of-art semantic segmentation network including Deeplab v3+ and PSPNet, and perform Meanshift clustering algorithm as a post-processing method to obtain the instance segmentation result. Moreover, our method achieves the state-of-art result on Tusimple lane detection benchmark dataset.</small></p>
                                                        </div>
                                                    </div>
                                                </td>
                                            </tr>
                                            -->
                                            <tr>
                                                    <td>
                                                        <p></p>
                                                        <div class="row vert-offset-top-1 vert-offset-bottom-1">
                                                            <div class="col-md-4">
                                                                <img src="./files/zoopt2.png" class="img-thumbnail center-block" style="max-width: 90%" >
                                                            </div>
                                                            <div class="col-md-75">
                                                                <p><b>Derivative-free Optimization(ZOOpt, zero-th order optimization)</b><br>
                                                                <small>Nanjing University, advised by <a href="http://www.lamda.nju.edu.cn/yuy/" target="_blank">Prof. Yang Yu</a>  
                                                                </small><br>
                                                                <p style="text-align:justify; max-width:97%"><small>Our research mainly focused on accomplishing the python packages:ZOOpt and ZOOsrv(a distributed version of ZOOpt),
                                                                        and designing comparative experiments with other packages used for optimization including CMA-ES and BayesOpt.
                                                                        Comparative experiments are designed to compare efficiency of optimization on testing functions such as sphere Prize
                                                                        function and ackley function and on clustering and classification.</small></p>
                                                            </div>
                                                        </div>
                                                    </td>
                                            </tr>
                                        </tbody>
                                    </table>
                                <!-- </div> -->
                                </div>
        
                                <hr size="1" color="#800000">
        <h2><b><a id="awards">Experiences</a></b></h2>
        <div class="row">
            <div class="col-xs-2 col-md-2"><p align="center">
                <strong>
                    03/2021 - 08/2021<br>
                </strong></p>
            </div>
            <div class="col-xs-18 col-md-15" style="margin-right: 5px"><p>
                Research Intern at <a target="_blank" href="https://www.msra.cn/">MSRA (Microsoft Research Asia)</a><br>
            </p></div>
        </div>

        <h2><b><a id="awards">Academic Service</a></b></h2>
        <ul>
            <li style="width: auto;font-size:140%">
            <b>Conference Reviewer</b>: <br /> 
        - Advances in Neural Information Processing Systems (NeurIPS)  <br /> 
        - International Conference on Machine Learning (ICML) <br /> 
        - International Conference on Learning Representations (ICLR) <br />  
        - IEEE Conference on Computer Vision and Pattern Recognition (CVPR) <br />
        - IEEE International Conference on Computer Vision (ICCV)   <br /> 
        - European Conference on Computer Vision (ECCV) <br />
        - British Machine Vision Conference (BMVC) <br />
        - Winter Conference on Applications of Computer Vision (WACV)
       
        </li>
        <li style="width: auto;font-size:140%">
            <b>Journal Reviewer</b>: <br /> 
        - IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI) <br />
        - IEEE Transactions on Image Processing (TIP) <br />
        - Springer - International Journal of Computer Vision (IJCV) <br />
        - Elsevier - Neurocomputing 
        
            </li>
    </ul>
        <!--
        <div class="row">
            <div class="col-xs-2 col-md-3"><p >
                <strong>
                    Conference Reviewer<br>
                    Journal Reviewer<br>
                </strong></p>
            </div>
            <div class="col-xs-18 col-md-15" style="margin-right: 5px"><p>
                CVPR, ICCV, ECCV, ICML, BMVC<br>
                TPAMI, TIP, Neurocomputing<br>
            </p></div>
        </div>
        -->
        <hr size="1" color="#800000">
        <h2><b><a id="awards">Awards & Competitions</a></b></h2>
        <div class="row">
            <div class="col-xs-2 col-md-2"><p align="center">
                <strong>
                    10/2021<br>
                    10/2021<br>
                    07/2021<br>
                    05/2021<br>
                    10/2020<br>
                    01/2020<br>
                    09/2019<br>
                    06/2019<br>
                    09/2018<br>
                    10/2016<br>
                    09/2014<br>
                </strong></p>
            </div>
            <div class="col-xs-18 col-md-15" style="margin-right: 5px"><p>
                Award for Academic Innovation, PKU<br>
                Merit Student of Peking University<br>
                Top 10 Outstanding Researcher (学术十杰), EECS, Peking University<br>
                HUAWEI Scholarship (top 2%)<br>
                Scholarship for Outstanding Research (top 5%)<br>
                1st National AI Challenge(Re-ID) Rank 20/2100<br>
                Peking University Scholarship<br>
                Outstanding Graduates of Nanjing University<br>
                Scholarship of Xingquan Fund<br>
                Second-class Scholarship for Outstanding Student <br>
                Second-Prize in High School Students Physics & Mathematics Contest in China
            </p></div>
        </div>



        <hr size="1" color="#800000">
        <h2><b><a name="misc">Misc</a></b></h2>

        <p>An INTJ. I love traveling and reading books.   </p>

        <p id="phd-day-count">It's been 1470 days since I started !<br>

        <script>
        var montharray=new Array("Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec")

        function countup(yr,m,d){
        var today=new Date()
        var todayy=today.getYear()
        if (todayy < 1000)
        todayy+=1900
        var todaym=today.getMonth()
        var todayd=today.getDate()
        var todaystring=montharray[todaym]+" "+todayd+", "+todayy
        var paststring=montharray[m-1]+" "+d+", "+yr
        var difference="It's been " + (Math.round((Date.parse(todaystring)-Date.parse(paststring))/(24*60*60*1000))*1) + " days since I started my PhD at CMU.<br>"
            document.getElementById('phd-day-count').value = difference
            document.getElementById('phd-day-count').innerHTML = difference
        }
        //enter the count up date using the format year/month/day
        countup(2022,08,15)
        </script>
        <script type="text/javascript" src="//rf.revolvermaps.com/0/0/8.js?i=5zvnamk8exm&amp;m=0&amp;c=ff0000&amp;cr1=ffffff&amp;f=arial&amp;l=33" async="async"></script>

        </p>
    </div>
</div><!--/.container-->

<footer class="footer">
    <div class="container" style="background-color: #ebebeb">
        <p></p>
        <p class="text-muted" align="center" style="color: rgb(51,51,51); line-height:18pt">
            Copyright &#169 Hanzhe Hu <script>var d = new Date(); document.write(d.getFullYear())</script>
        </p>
        <p class="text-muted" align="center" style="color: rgb(51,51,51); line-height:12pt">
            <small><small>
                <script>
                    var lastModTime = new Date(document.lastModified);
                    document.write("Last Modified: "+lastModTime)
                </script><br>
            
            </small></small>   
        </p>


       
    </div>
</footer>


<script src="./files/jquery-2.1.4.min.js"></script>
<script src="./files/bootstrap.min.js"></script>


</body></html>
